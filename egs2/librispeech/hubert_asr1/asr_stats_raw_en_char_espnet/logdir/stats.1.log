# Running on r097.ib.bridges2.psc.edu
# Started at Thu Jun 17 18:25:43 EDT 2021
# SLURMD_NODENAME=r097
# SLURM_ARRAY_JOB_ID=1724655
# SLURM_ARRAY_TASK_COUNT=4
# SLURM_ARRAY_TASK_ID=1
# SLURM_ARRAY_TASK_MAX=4
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_EXPORT_ENV=PATH
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=1724656
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=1724656
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r097
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=80925
# SLURM_JOB_USER=jzmo
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=2000
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r097
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/jzmo/work/espnet/egs2/librispeech/hubert_asr1
# SLURM_SUBMIT_HOST=br014.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=3772881
# SLURM_TOPOLOGY_ADDR=r097
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6814:9216:109
# python3 -m espnet2.bin.asr_train --collect_stats true --use_preprocessor true --bpemodel none --token_type char --token_list data/en_token_list/char/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --train_data_path_and_name_and_type dump/raw/train_10h/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train_10h/text,text,text --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --train_shape_file asr_stats_raw_en_char_espnet/logdir/train.1.scp --valid_shape_file asr_stats_raw_en_char_espnet/logdir/valid.1.scp --output_dir asr_stats_raw_en_char_espnet/logdir/stats.1 --config conf/tuning/train_asr_hubert_base_10h_finetuning_espnet.yaml --frontend_conf fs=16k 
/ocean/projects/cis210027p/jzmo/work/espnet/tools/anaconda/envs/espnet/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: "sox" backend is being deprecated. The default backend will be changed to "sox_io" backend in 0.8.0 and "sox" backend will be removed in 0.9.0. Please migrate to "sox_io" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.
  warnings.warn(
/ocean/projects/cis210027p/jzmo/work/espnet/tools/anaconda/envs/espnet/bin/python3 /ocean/projects/cis210027p/jzmo/work/espnet/espnet2/bin/asr_train.py --collect_stats true --use_preprocessor true --bpemodel none --token_type char --token_list data/en_token_list/char/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --train_data_path_and_name_and_type dump/raw/train_10h/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train_10h/text,text,text --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --train_shape_file asr_stats_raw_en_char_espnet/logdir/train.1.scp --valid_shape_file asr_stats_raw_en_char_espnet/logdir/valid.1.scp --output_dir asr_stats_raw_en_char_espnet/logdir/stats.1 --config conf/tuning/train_asr_hubert_base_10h_finetuning_espnet.yaml --frontend_conf fs=16k
[r097] 2021-06-17 18:27:17,640 (asr:361) INFO: Vocabulary size: 31
/ocean/projects/cis210027p/jzmo/work/espnet/tools/anaconda/envs/espnet/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729062494/work/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
[r097] 2021-06-17 18:27:26,779 (hubert:230) INFO: HubertModel Config: HubertConfig(_name=None, label_rate=100, extractor_mode='default', encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_fn='gelu', dropout=0.1, attention_dropout=0.0, activation_dropout=0.0, encoder_layerdrop=0.05, dropout_input=0.1, dropout_features=0.1, final_dim=256, untie_final_proj=True, layer_norm_first=False, conv_feature_layers='[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', conv_bias=False, logit_temp=0.1, target_glu=False, feature_grad_mult=0.1, mask_length=10, mask_prob=0.8, mask_selection='static', mask_other=0, no_mask_overlap=False, mask_min_space=1, mask_channel_length=10, mask_channel_prob=0.0, mask_channel_selection='static', mask_channel_other=0, no_mask_channel_overlap=False, mask_channel_min_space=1, conv_pos=128, conv_pos_groups=16, latent_temp=(2, 0.5, 0.999995), skip_masked=False, skip_nomask=False)
[r097] 2021-06-17 18:27:29,763 (hubert_encoder:221) INFO: Pretrained Hubert model parameters reloaded!
[r097] 2021-06-17 18:27:29,893 (abs_task:1097) INFO: Setting encoder.encoders.mask_emb.requires_grad = False
[r097] 2021-06-17 18:27:29,894 (abs_task:1097) INFO: Setting encoder.encoders.feature_extractor.conv_layers.0.0.weight.requires_grad = False
[r097] 2021-06-17 18:27:29,894 (abs_task:1097) INFO: Setting encoder.encoders.feature_extractor.conv_layers.0.2.weight.requires_grad = False
[r097] 2021-06-17 18:27:29,894 (abs_task:1097) INFO: Setting encoder.encoders.feature_extractor.conv_layers.0.2.bias.requires_grad = False
[r097] 2021-06-17 18:27:29,894 (abs_task:1097) INFO: Setting encoder.encoders.feature_extractor.conv_layers.1.0.weight.requires_grad = False
[r097] 2021-06-17 18:27:29,894 (abs_task:1097) INFO: Setting encoder.encoders.feature_extractor.conv_layers.2.0.weight.requires_grad = False
[r097] 2021-06-17 18:27:29,894 (abs_task:1097) INFO: Setting encoder.encoders.feature_extractor.conv_layers.3.0.weight.requires_grad = False
[r097] 2021-06-17 18:27:29,894 (abs_task:1097) INFO: Setting encoder.encoders.feature_extractor.conv_layers.4.0.weight.requires_grad = False
[r097] 2021-06-17 18:27:29,894 (abs_task:1097) INFO: Setting encoder.encoders.feature_extractor.conv_layers.5.0.weight.requires_grad = False
[r097] 2021-06-17 18:27:29,894 (abs_task:1097) INFO: Setting encoder.encoders.feature_extractor.conv_layers.6.0.weight.requires_grad = False
[r097] 2021-06-17 18:27:29,895 (abs_task:1097) INFO: Setting encoder.encoders.post_extract_proj.weight.requires_grad = False
[r097] 2021-06-17 18:27:29,895 (abs_task:1097) INFO: Setting encoder.encoders.post_extract_proj.bias.requires_grad = False
[r097] 2021-06-17 18:27:29,897 (abs_task:1097) INFO: Setting encoder.encoders.encoder.pos_conv.0.bias.requires_grad = False
[r097] 2021-06-17 18:27:29,897 (abs_task:1097) INFO: Setting encoder.encoders.encoder.pos_conv.0.weight_g.requires_grad = False
[r097] 2021-06-17 18:27:29,897 (abs_task:1097) INFO: Setting encoder.encoders.encoder.pos_conv.0.weight_v.requires_grad = False
[r097] 2021-06-17 18:27:29,918 (abs_task:1121) INFO: pytorch.version=1.7.0, cuda.available=False, cudnn.version=7605, cudnn.benchmark=False, cudnn.deterministic=True
[r097] 2021-06-17 18:27:29,929 (abs_task:1122) INFO: Model structure:
ESPnetASRModel(
  (encoder): FairseqHubertEncoder(
    (encoders): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU()
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.1, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU()
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): Linear(in_features=768, out_features=256, bias=True)
    )
    (output_layer): Sequential(
      (0): Linear(in_features=768, out_features=1024, bias=True)
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=1024, out_features=31, bias=True)
    (ctc_loss): CTCLoss()
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 95.40 M
    Number of trainable parameters: 86.09 M (90.2%)
    Size: 344.35 MB
    Type: torch.float32
[r097] 2021-06-17 18:27:29,946 (abs_task:1125) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 2e-05
    lr: 2e-08
    weight_decay: 0
)
[r097] 2021-06-17 18:27:29,946 (abs_task:1126) INFO: Scheduler: WarmupLR(warmup_steps=1000)
[r097] 2021-06-17 18:27:29,947 (abs_task:1135) INFO: Saving the configuration in asr_stats_raw_en_char_espnet/logdir/stats.1/config.yaml
[r097] 2021-06-17 18:27:29,956 (abs_task:1160) INFO: Namespace(accum_grad=1, allow_variable_data_keys=False, batch_bins=10000000, batch_size=20, batch_type='numel', best_model_criterion=[['valid', 'loss', 'min']], bpemodel=None, chunk_length=500, chunk_shift_ratio=0.5, cleaner=None, collect_stats=True, config='conf/tuning/train_asr_hubert_base_10h_finetuning_espnet.yaml', ctc_conf={'dropout_rate': 0.0, 'ctc_type': 'builtin', 'reduce': True, 'ignore_nan_grad': False}, cudnn_benchmark=False, cudnn_deterministic=True, cudnn_enabled=True, decoder='rnn', decoder_conf={}, detect_anomaly=False, dist_backend='nccl', dist_init_method='env://', dist_launcher=None, dist_master_addr=None, dist_master_port=None, dist_rank=None, dist_world_size=None, distributed=False, dry_run=False, early_stopping_criterion=('valid', 'loss', 'min'), encoder='hubert', encoder_conf={'output_size': 1024, 'normalize_before': False, 'freeze_finetune_updates': 1000, 'hubert_url': 'espnet', 'hubert_dir_path': 'exp/asr_train_asr_hubert_base_960h_pretrain_raw_en_word/'}, fold_length=[], freeze_param=['encoder.encoders.mask_emb', 'encoder.encoders.feature_extractor', 'encoder.encoders.post_extract_proj', 'encoder.encoders.encoder.pos_conv'], frontend=None, frontend_conf={}, g2p=None, grad_clip=5.0, grad_clip_type=2.0, grad_noise=False, ignore_init_mismatch=False, init='xavier_uniform', init_param=[], input_size=1, iterator_type='sequence', keep_nbest_models=10, local_rank=None, log_interval=None, log_level='INFO', max_cache_fd=32, max_cache_size=0.0, max_epoch=200, model_conf={'ctc_weight': 1.0, 'lsm_weight': 0.1, 'length_normalized_loss': False}, multiple_iterator=False, multiprocessing_distributed=False, ngpu=0, no_forward_run=False, noise_apply_prob=1.0, noise_db_range='13_15', noise_scp=None, non_linguistic_symbols=None, normalize=None, normalize_conf={}, num_att_plot=3, num_cache_chunks=1024, num_iters_per_epoch=None, num_workers=1, optim='adam', optim_conf={'lr': 2e-05}, output_dir='asr_stats_raw_en_char_espnet/logdir/stats.1', patience=None, preencoder=None, preencoder_conf={}, pretrain_path=None, print_config=False, required=['output_dir', 'token_list'], resume=False, rir_apply_prob=1.0, rir_scp=None, scheduler='warmuplr', scheduler_conf={'warmup_steps': 1000}, seed=0, sharded_ddp=False, sort_batch='descending', sort_in_batch='descending', specaug=None, specaug_conf={}, speech_volume_normalize=None, token_list=['<blank>', '<unk>', '<space>', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', "'", 'X', 'J', 'Q', 'Z', '<sos/eos>'], token_type='char', train_data_path_and_name_and_type=[('dump/raw/train_10h/wav.scp', 'speech', 'sound'), ('dump/raw/train_10h/text', 'text', 'text')], train_dtype='float32', train_shape_file=['asr_stats_raw_en_char_espnet/logdir/train.1.scp'], unused_parameters=True, use_amp=False, use_preprocessor=True, use_tensorboard=True, use_wandb=False, val_scheduler_criterion=('valid', 'loss'), valid_batch_bins=None, valid_batch_size=None, valid_batch_type=None, valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/text', 'text', 'text')], valid_max_cache_size=None, valid_shape_file=['asr_stats_raw_en_char_espnet/logdir/valid.1.scp'], version='0.9.10', wandb_id=None, wandb_project=None, write_collected_feats=False)
# Accounting: begin_time=1623968743
# Accounting: end_time=1623968901
# Accounting: time=158 threads=1
# Finished at Thu Jun 17 18:28:21 EDT 2021 with status 0
